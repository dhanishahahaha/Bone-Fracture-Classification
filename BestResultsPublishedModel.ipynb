{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8252823,"sourceType":"datasetVersion","datasetId":4897098},{"sourceId":182065477,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1102.022695,"end_time":"2024-06-11T14:52:04.823254","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-11T14:33:42.800559","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shyamgupta196/efficient-net-transfer-learning?scriptVersionId=184591643\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pycocotools\n","metadata":{"papermill":{"duration":13.943461,"end_time":"2024-06-11T14:33:59.687799","exception":false,"start_time":"2024-06-11T14:33:45.744338","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:47:42.351143Z","iopub.execute_input":"2024-06-20T15:47:42.351893Z","iopub.status.idle":"2024-06-20T15:47:44.788215Z","shell.execute_reply.started":"2024-06-20T15:47:42.351862Z","shell.execute_reply":"2024-06-20T15:47:44.787085Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.8)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install efficientnet-pytorch","metadata":{"papermill":{"duration":15.162011,"end_time":"2024-06-11T14:34:14.855368","exception":false,"start_time":"2024-06-11T14:33:59.693357","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:47:44.790414Z","iopub.execute_input":"2024-06-20T15:47:44.790699Z","iopub.status.idle":"2024-06-20T15:47:56.673597Z","shell.execute_reply.started":"2024-06-20T15:47:44.790675Z","shell.execute_reply":"2024-06-20T15:47:56.672658Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet-pytorch in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom tqdm import tqdm\nfrom pycocotools.coco import COCO\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline\nfrom efficientnet_pytorch import EfficientNet\n","metadata":{"papermill":{"duration":5.254563,"end_time":"2024-06-11T14:34:20.11634","exception":false,"start_time":"2024-06-11T14:34:14.861777","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:47:56.675044Z","iopub.execute_input":"2024-06-20T15:47:56.675359Z","iopub.status.idle":"2024-06-20T15:47:56.685362Z","shell.execute_reply.started":"2024-06-20T15:47:56.675331Z","shell.execute_reply":"2024-06-20T15:47:56.684539Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"import sys\nimport wandb\nsys.path.append('/kaggle/usr/lib/wandblogin/')\nfrom wandblogin import login\nlogin()\n\n# # Initialize wandb\nwandb.init(project=\"fracatlas--transferlearning\")","metadata":{"papermill":{"duration":18.400812,"end_time":"2024-06-11T14:34:38.523573","exception":false,"start_time":"2024-06-11T14:34:20.122761","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:58:25.229639Z","iopub.execute_input":"2024-06-20T15:58:25.230368Z","iopub.status.idle":"2024-06-20T15:59:27.873113Z","shell.execute_reply.started":"2024-06-20T15:58:25.230336Z","shell.execute_reply":"2024-06-20T15:59:27.87225Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240620_155825-5oy06lxb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning/runs/5oy06lxb' target=\"_blank\">azure-monkey-1</a></strong> to <a href='https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning' target=\"_blank\">https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning/runs/5oy06lxb' target=\"_blank\">https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning/runs/5oy06lxb</a>"},"metadata":{}},{"execution_count":51,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shyam-d-gupta/fracatlas--transferlearning/runs/5oy06lxb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79b0703b31f0>"},"metadata":{}}]},{"cell_type":"code","source":"coco = COCO('/kaggle/input/fracatlas-original-dataset/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json')\nimg_dir = '/kaggle/input/fracatlas-original-dataset/FracAtlas/images/Fractured'\nimage_id = 12\n\nimg = coco.imgs[image_id]\n","metadata":{"papermill":{"duration":0.054655,"end_time":"2024-06-11T14:34:38.585489","exception":false,"start_time":"2024-06-11T14:34:38.530834","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.574889Z","iopub.status.idle":"2024-06-20T15:48:05.575392Z","shell.execute_reply.started":"2024-06-20T15:48:05.575126Z","shell.execute_reply":"2024-06-20T15:48:05.575145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = np.array(Image.open(os.path.join(img_dir, img['file_name'])))\nplt.imshow(image, interpolation='nearest')\nplt.show()\n","metadata":{"papermill":{"duration":0.489099,"end_time":"2024-06-11T14:34:39.081817","exception":false,"start_time":"2024-06-11T14:34:38.592718","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.576734Z","iopub.status.idle":"2024-06-20T15:48:05.577186Z","shell.execute_reply.started":"2024-06-20T15:48:05.576958Z","shell.execute_reply":"2024-06-20T15:48:05.576978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image)\ncat_ids = coco.getCatIds()\nanns_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None)\nanns = coco.loadAnns(anns_ids)\ncoco.showAnns(anns)","metadata":{"papermill":{"duration":0.791905,"end_time":"2024-06-11T14:34:39.883101","exception":false,"start_time":"2024-06-11T14:34:39.091196","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.578709Z","iopub.status.idle":"2024-06-20T15:48:05.579143Z","shell.execute_reply.started":"2024-06-20T15:48:05.578921Z","shell.execute_reply":"2024-06-20T15:48:05.578939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = coco.annToMask(anns[0])\nfor i in range(len(anns)):\n    mask += coco.annToMask(anns[i])\n\nplt.imshow(mask,cmap='gray')","metadata":{"papermill":{"duration":0.835613,"end_time":"2024-06-11T14:34:40.729751","exception":false,"start_time":"2024-06-11T14:34:39.894138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.580543Z","iopub.status.idle":"2024-06-20T15:48:05.580907Z","shell.execute_reply.started":"2024-06-20T15:48:05.580746Z","shell.execute_reply":"2024-06-20T15:48:05.580761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncoco = COCO('/kaggle/input/fracatlas-original-dataset/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json')\n\nimg_dir = '/kaggle/input/fracatlas-original-dataset/FracAtlas/images/Fractured'\noutput_dir = '/kaggle/working/fracatlas/images/fractured_masks/'\n\nos.makedirs(output_dir, exist_ok=True)\n\nimage_ids = coco.getImgIds()\nnum_images =14\nfor i in range(num_images):    \n    img = coco.loadImgs(image_ids[i])[0]\n    image = np.array(Image.open(os.path.join(img_dir, img['file_name'])))\n\n    \n\n    cat_ids = coco.getCatIds()\n    anns_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None)\n    anns = coco.loadAnns(anns_ids)\n\n    mask = coco.annToMask(anns[0])\n    for j in range(1, len(anns)):\n        mask += coco.annToMask(anns[j])\n        \n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title('Original Image')\n    \n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(image)\n    coco.showAnns(anns)\n    plt.axis('off')\n    plt.title('Annotations')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(mask, cmap='gray')\n    plt.axis('off')\n    plt.title('Combined Mask')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"papermill":{"duration":21.857213,"end_time":"2024-06-11T14:35:02.598545","exception":false,"start_time":"2024-06-11T14:34:40.741332","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.583474Z","iopub.status.idle":"2024-06-20T15:48:05.584307Z","shell.execute_reply.started":"2024-06-20T15:48:05.584054Z","shell.execute_reply":"2024-06-20T15:48:05.584073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nimage_folder_fractured = '/kaggle/input/fracatlas-original-dataset/FracAtlas/images/Fractured'\nimage_folder_non_fractured = '/kaggle/input/fracatlas-original-dataset/FracAtlas/images/Non_fractured'\ntrain_folder = '/kaggle/working/train'\nval_folder = '/kaggle/working/val'\ntest_folder = '/kaggle/working/test'\n\nos.makedirs(train_folder + '/fractured', exist_ok=True)\nos.makedirs(train_folder + '/non_fractured', exist_ok=True)\nos.makedirs(val_folder + '/fractured', exist_ok=True)\nos.makedirs(val_folder + '/non_fractured', exist_ok=True)\nos.makedirs(test_folder + '/fractured', exist_ok=True)\nos.makedirs(test_folder + '/non_fractured', exist_ok=True)\n\ndef split_images(image_folder, train_folder, val_folder, test_folder, split_ratio=(0.8, 0.1, 0.1)):\n    image_files = os.listdir(image_folder)\n    random.shuffle(image_files)\n    num_images = len(image_files)\n    num_train = int(split_ratio[0] * num_images)\n    num_val = int(split_ratio[1] * num_images)\n\n    train_files = image_files[:num_train]\n    val_files = image_files[num_train:num_train + num_val]\n    test_files = image_files[num_train + num_val:]\n\n    for file in train_files:\n        shutil.copy(os.path.join(image_folder, file), os.path.join(train_folder, file))\n    for file in val_files:\n        shutil.copy(os.path.join(image_folder, file), os.path.join(val_folder, file))\n    for file in test_files:\n        shutil.copy(os.path.join(image_folder, file), os.path.join(test_folder, file))\n\nsplit_images(image_folder_fractured, train_folder + '/fractured', val_folder + '/fractured', test_folder + '/fractured')\nsplit_images(image_folder_non_fractured, train_folder + '/non_fractured', val_folder + '/non_fractured', test_folder + '/non_fractured')\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n#     transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_folder, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nval_dataset = torchvision.datasets.ImageFolder(val_folder, transform=transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n\ntest_dataset = torchvision.datasets.ImageFolder(test_folder, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"papermill":{"duration":24.685589,"end_time":"2024-06-11T14:35:27.34786","exception":false,"start_time":"2024-06-11T14:35:02.662271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.58614Z","iopub.status.idle":"2024-06-20T15:48:05.586612Z","shell.execute_reply.started":"2024-06-20T15:48:05.586383Z","shell.execute_reply":"2024-06-20T15:48:05.586403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:48:05.587976Z","iopub.status.idle":"2024-06-20T15:48:05.589123Z","shell.execute_reply.started":"2024-06-20T15:48:05.588882Z","shell.execute_reply":"2024-06-20T15:48:05.588902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = EfficientNet.from_pretrained('efficientnet-b6')\nnum_ftrs = model._fc.in_features\nmodel._fc = nn.Linear(num_ftrs, 2)  # Assuming binary classification\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"papermill":{"duration":1.717783,"end_time":"2024-06-11T14:35:29.131881","exception":false,"start_time":"2024-06-11T14:35:27.414098","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:48:05.590534Z","iopub.status.idle":"2024-06-20T15:48:05.591271Z","shell.execute_reply.started":"2024-06-20T15:48:05.591031Z","shell.execute_reply":"2024-06-20T15:48:05.59105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(all_labels, all_predictions)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:48:06.785539Z","iopub.execute_input":"2024-06-20T15:48:06.785881Z","iopub.status.idle":"2024-06-20T15:48:06.797582Z","shell.execute_reply.started":"2024-06-20T15:48:06.785854Z","shell.execute_reply":"2024-06-20T15:48:06.796363Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0.5029769258827821"},"metadata":{}}]},{"cell_type":"code","source":"from PIL import ImageFile\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nimport wandb\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nbest_val_acc = 0.0\n\n# Log initial configuration\nwandb.config = {\n    \"learning_rate\": 0.001,\n    \"epochs\": 13,\n    \"batch_size\": 150\n}\n\nnum_epochs = 13\nfor epoch in range(num_epochs):\n    model.train()\n    torch.cuda.empty_cache()\n    running_loss = 0.0\n    for i, data in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'), 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Log training metrics\n    wandb.log({\"epoch\": epoch+1, \"train_loss\": running_loss/len(train_loader)})\n\n    model.eval()\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predictions = []\n    with torch.no_grad():\n        for data in val_loader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n            # Log validation images\n            if i == 0:\n                wandb.log({\"examples\": [wandb.Image(img) for img in images.cpu()]})\n\n    val_acc = correct / total\n    precision = precision_score(all_labels, all_predictions, average='macro')\n    recall = recall_score(all_labels, all_predictions, average='macro')\n    f1 = f1_score(all_labels, all_predictions, average='macro')\n    cm = confusion_matrix(all_labels, all_predictions)\n    auc_roc = roc_auc_score(all_labels, all_predictions)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {val_acc*100:.2f}%')\n    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, AUC ROC: {auc_roc:.4f}')\n\n    # Log validation metrics\n    wandb.log({\n        \"val_accuracy\": val_acc,\n        \"val_precision\": precision,\n        \"val_recall\": recall,\n        \"val_f1_score\": f1,\n        \"val_auc_roc\": auc_roc,\n        \"val_confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n                                                             y_true=all_labels, \n                                                             preds=all_predictions,\n                                                             class_names=[str(i) for i in range(outputs.size(1))])\n    })\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        print(\"saving...\")\n        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n\nprint('Finished Training')\n","metadata":{"papermill":{"duration":985.807939,"end_time":"2024-06-11T14:51:55.051183","exception":false,"start_time":"2024-06-11T14:35:29.243244","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T16:03:42.312619Z","iopub.execute_input":"2024-06-20T16:03:42.313255Z","iopub.status.idle":"2024-06-20T16:33:35.918829Z","shell.execute_reply.started":"2024-06-20T16:03:42.313224Z","shell.execute_reply":"2024-06-20T16:33:35.916315Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 244/244 [02:14<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 0.3173, Validation Accuracy: 67.23%\nPrecision: 0.6383, Recall: 0.7395, F1 Score: 0.6162, AUC ROC: 0.7395\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20, Loss: 0.2589, Validation Accuracy: 88.47%\nPrecision: 0.9389, Recall: 0.6654, F1 Score: 0.7160, AUC ROC: 0.6654\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20, Loss: 0.1987, Validation Accuracy: 93.26%\nPrecision: 0.9470, Recall: 0.8134, F1 Score: 0.8622, AUC ROC: 0.8134\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 244/244 [02:14<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20, Loss: 0.1483, Validation Accuracy: 96.63%\nPrecision: 0.9482, Recall: 0.9320, F1 Score: 0.9399, AUC ROC: 0.9320\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20, Loss: 0.1301, Validation Accuracy: 91.58%\nPrecision: 0.8426, Recall: 0.8836, F1 Score: 0.8609, AUC ROC: 0.8836\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 244/244 [02:14<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20, Loss: 0.1093, Validation Accuracy: 97.80%\nPrecision: 0.9802, Recall: 0.9420, F1 Score: 0.9598, AUC ROC: 0.9420\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 244/244 [02:14<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20, Loss: 0.0864, Validation Accuracy: 98.32%\nPrecision: 0.9718, Recall: 0.9690, F1 Score: 0.9704, AUC ROC: 0.9690\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\nEpoch 10/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20, Loss: 0.0536, Validation Accuracy: 93.52%\nPrecision: 0.8691, Recall: 0.9311, F1 Score: 0.8955, AUC ROC: 0.9311\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 244/244 [02:14<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20, Loss: 0.0705, Validation Accuracy: 97.67%\nPrecision: 0.9863, Recall: 0.9323, F1 Score: 0.9568, AUC ROC: 0.9323\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 244/244 [02:14<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20, Loss: 0.0482, Validation Accuracy: 99.09%\nPrecision: 0.9914, Recall: 0.9767, F1 Score: 0.9839, AUC ROC: 0.9767\nsaving...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20:  19%|█▉        | 47/244 [00:25<01:48,  1.82it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     24\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2186\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2190\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2191\u001b[0m         )\n\u001b[0;32m-> 2193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"\n\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\nmodel.eval()\ncorrect = 0\ntotal = 0\nall_labels = []\nall_predictions = []\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\ntest_accuracy = correct / total\nprecision = precision_score(all_labels, all_predictions, average='macro')\nrecall = recall_score(all_labels, all_predictions, average='macro')\nf1 = f1_score(all_labels, all_predictions, average='macro')\ncm = confusion_matrix(all_labels, all_predictions)\nauc_roc = roc_auc_score(all_labels, all_predictions)\n\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\nprint(f'Test Precision: {precision:.4f}, Test Recall: {recall:.4f}, Test F1 Score: {f1:.4f}, Test AUC ROC: {auc_roc:.4f}')\n\n# Log final test metrics\nwandb.log({\n    \"test_accuracy\": test_accuracy,\n    \"test_precision\": precision,\n    \"test_recall\": recall,\n    \"test_f1_score\": f1,\n    \"test_auc_roc\": auc_roc,\n    \"test_confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n                                                          y_true=all_labels, \n                                                          preds=all_predictions,\n                                                          class_names=[str(i) for i in range(outputs.size(1))])\n})\n\nwandb.finish()\n","metadata":{"papermill":{"duration":6.551089,"end_time":"2024-06-11T14:52:01.754298","exception":false,"start_time":"2024-06-11T14:51:55.203209","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T16:34:09.102358Z","iopub.execute_input":"2024-06-20T16:34:09.102794Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Test Accuracy: 98.18%\nTest Precision: 0.9770, Test Recall: 0.9606, Test F1 Score: 0.9686, Test AUC ROC: 0.9606\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.148521,"end_time":"2024-06-11T14:52:02.052179","exception":false,"start_time":"2024-06-11T14:52:01.903658","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}